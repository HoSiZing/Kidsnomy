import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from peft import get_peft_model, LoraConfig, TaskType
from datasets import Dataset
from sklearn.model_selection import train_test_split
import json
import pandas as pd

# ì¹´í…Œê³ ë¦¬ ëª©ë¡ê³¼ ë§¤í•‘
categories = ['êµìœ¡', 'ì˜ë£Œ', 'ì·¨ë¯¸', 'ì‹ë¹„', 'ì¹´í˜/ê°„ì‹', 'ì‡¼í•‘', 'ë¬¸êµ¬ì ', 'ë¯¸ìš©', 'ë¬¸í™”', 'ë„ì„œ', 'ìƒí™œ', 'êµí†µ']
eng_categories = ['edu', 'health', 'hobby', 'eat', 'cafe', 'shopping', 'stationary', 'beauty', 'culture', 'book', 'life', 'transportation']
category_to_id = {cat: i for i, cat in enumerate(categories)}

# í•™ìŠµ ë°ì´í„°ë¡œ ë°”ê¾¸ê¸°
def preprocess_data():
    # ìµœì¢… í•™ìŠµ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    all_data = []

    # eng_categories ë¦¬ìŠ¤íŠ¸

    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    # for eng_cate in eng_categories:
    #     with open(f"./preprocessed/preprocess_data_{eng_cate}.json", "r", encoding="utf-8") as f:
    #         data = json.load(f)
    with open(f"./preprocessed/A_final_total_data.json", "r", encoding="utf-8") as f:
        data = json.load(f)
        # ë°ì´í„° ì „ì²˜ë¦¬: ê° í•­ëª©ì— ëŒ€í•´ ì¹´í…Œê³ ë¦¬ ë²ˆí˜¸ì™€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ
        for item in data:
            category_name = item.get("category_name", "")  # í…ìŠ¤íŠ¸ ë‚´ìš©
            # place_name = item.get("place_name", "")
            label = category_to_id.get(item.get("label", ""), -1)  # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ìˆ«ì ë¼ë²¨ (ì—†ìœ¼ë©´ -1)
            all_data.append({"category_name": category_name, "label": label})

    # ê²°ê³¼ í™•ì¸
    return all_data

# ì „ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°ë¥¼ ì–»ê¸°
data = preprocess_data()

# JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
with open("processed_data.json", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

print(f"Data has been saved as processed_data.json.")

texts = [item['category_name'] for item in data]
labels = [item['label'] for item in data]  # 'label'ë¡œ ìˆ˜ì •

# train, validation ë°ì´í„°ë¡œ ë¶„í• 
train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)

# ê²°ê³¼ í™•ì¸
print(f"Train texts: {len(train_texts)}")
print(f"Validation texts: {len(val_texts)}")

# í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "skt/kobert-base-v1"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# í† í°í™” í•¨ìˆ˜
def tokenize_function(texts):
    return tokenizer(texts, padding="max_length", truncation=True, max_length=64)

# í† í°í™” ì ìš©
train_encodings = tokenize_function(train_texts)
val_encodings = tokenize_function(val_texts)

# ë°ì´í„°ì…‹ ë³€í™˜
train_dataset = Dataset.from_dict({"input_ids": train_encodings["input_ids"], "attention_mask": train_encodings["attention_mask"], "labels": train_labels})
val_dataset = Dataset.from_dict({"input_ids": val_encodings["input_ids"], "attention_mask": val_encodings["attention_mask"], "labels": val_labels})

# í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "skt/kobert-base-v1"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# í† í°í™” í•¨ìˆ˜
def tokenize_function(texts):
    return tokenizer(texts, padding="max_length", truncation=True, max_length=64)

# í† í°í™” ì ìš©
train_encodings = tokenize_function(train_texts)
val_encodings = tokenize_function(val_texts)

# ë°ì´í„°ì…‹ ë³€í™˜
train_dataset = Dataset.from_dict({"input_ids": train_encodings["input_ids"], "attention_mask": train_encodings["attention_mask"], "labels": train_labels})
val_dataset = Dataset.from_dict({"input_ids": val_encodings["input_ids"], "attention_mask": val_encodings["attention_mask"], "labels": val_labels})

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (LoRA ì ìš©)
base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(categories))

lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,  # LoRA ë­í¬
    lora_alpha=32,
    lora_dropout=0.2
)

model = get_peft_model(base_model, lora_config)

# ëª¨ë¸ GPUë¡œ ì´ë™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./kobert_lora_category",
    eval_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    learning_rate=2e-5,
    logging_dir="./logs",
    logging_steps=50
)

# Trainer ì„¤ì •
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# ëª¨ë¸ í•™ìŠµ
trainer.train()
'''
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
from peft import get_peft_model, LoraConfig, TaskType
from datasets import Dataset
from sklearn.model_selection import train_test_split
import json

# ì¹´í…Œê³ ë¦¬ ëª©ë¡ê³¼ ë§¤í•‘
categories = ['êµìœ¡', 'ì˜ë£Œ', 'ì·¨ë¯¸', 'ì‹ë¹„', 'ì¹´í˜/ê°„ì‹', 'ì‡¼í•‘', 'ë¬¸êµ¬ì ', 'ë¯¸ìš©', 'ë¬¸í™”', 'ë„ì„œ', 'ìƒí™œ', 'êµí†µ']
category_to_id = {cat: i for i, cat in enumerate(categories)}

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
with open("./preprocessed/A_final_total_data.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# ë°ì´í„° ì „ì²˜ë¦¬ (ì˜ëª»ëœ ë¼ë²¨ ì œê±°)
filtered_data = [{"category_name": item["category_name"], "label": category_to_id[item["label"]]}
                 for item in data if item["label"] in category_to_id]

texts = [item['category_name'] for item in filtered_data]
labels = [item['label'] for item in filtered_data]

# train, validation ë°ì´í„°ë¡œ ë¶„í• 
train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "skt/kobert-base-v1"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# í† í°í™” í•¨ìˆ˜
def tokenize_function(texts):
    return tokenizer(texts, padding="max_length", truncation=True, max_length=64)

# í† í°í™” ì ìš©
train_encodings = tokenize_function(train_texts)
val_encodings = tokenize_function(val_texts)

# ë°ì´í„°ì…‹ ë³€í™˜ (ğŸš¨ labelsë¥¼ intë¡œ ë³€í™˜ í•„ìˆ˜)
train_dataset = Dataset.from_dict({
    "input_ids": train_encodings["input_ids"],
    "attention_mask": train_encodings["attention_mask"],
    "labels": list(map(int, train_labels))
})

val_dataset = Dataset.from_dict({
    "input_ids": val_encodings["input_ids"],
    "attention_mask": val_encodings["attention_mask"],
    "labels": list(map(int, val_labels))
})

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (LoRA ì ìš©)
base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(categories))

# BERT ê°€ì¤‘ì¹˜ ê³ ì •
for param in base_model.parameters():
    param.requires_grad = False

# LoRA ì„¤ì •
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.2
)

model = get_peft_model(base_model, lora_config)

# LoRA ê°€ì¤‘ì¹˜ë§Œ í•™ìŠµ
for name, param in model.named_parameters():
    param.requires_grad = "lora" in name

# ëª¨ë¸ GPUë¡œ ì´ë™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸ (ğŸš¨ LoRAë§Œ í•™ìŠµí•´ì•¼ ì •ìƒ)
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable parameters: {trainable_params} / {total_params}")

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./kobert_lora_category",
    eval_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    learning_rate=2e-5,
    logging_dir="./logs",
    logging_steps=50,
    optim="adamw_torch"
)

# ë°ì´í„° ë¡œë” ì¶”ê°€ (ğŸš¨ HuggingFace Trainerì— ë§ê²Œ ì„¤ì •)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Trainer ì„¤ì •
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator
)

# ëª¨ë¸ í•™ìŠµ
trainer.train()
'''