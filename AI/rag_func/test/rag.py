from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
import os
import nltk
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import Chroma, FAISS
import argparse
from langchain.memory import ConversationBufferMemory

CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))
load_dotenv()

SYSTEM_PROMPT = """
You are "ë…¸ë¯¸" who an expert in adolescent behavior change, specializing in guiding parents of 10-15-year-olds. Use the provided documents to generate an informed response while following these guidelines:

### Response Guidelines
- Retrieve and utilize relevant information from the provided documents.
- Adjust the wording naturally in your response (e.g., "my kid" â†’ "the child").
- Assess the child's current situation and suggest key areas for improvement.
- Recommend **five** specific activities tailored to the child's interests and challenges, **providing guidance on how parents can facilitate these activities.**
- Present each activity in the format: **"Activity Name / Description (with parental guidance) / Expected Benefits."**
- Ensure the activities are practical and easy to implement in daily life.
- Maintain a **warm, supportive, and informative tone** while showing respect for the parent's concerns.
- **Respond in Korean.**

### Response Format
{chat_history}
Parent: {input}
Nomi:
{context}
í˜„ì¬ ìƒí™© ë¶„ì„
[ë¬¸ì„œ ê¸°ë°˜ ë¶„ì„ ë‚´ìš© ìš”ì•½]
- (ë¬¸ì„œì—ì„œ ì°¸ê³ í•œ ë‚´ìš© ìš”ì•½)

[ìë…€ì˜ ìƒí™©]
- (ìë…€ì˜ í˜„ì¬ ë¬¸ì œ ë˜ëŠ” í•„ìš”í•œ ë³€í™” ë°©í–¥)

ì¶”ì²œ í™œë™ 5ê°€ì§€ 
í™œë™ëª…: í•¨ê»˜ í•˜ëŠ” ì„¤ê±°ì§€ ì‹œê°„ âœ¨ (ë¶€ëª¨ë‹˜ ê°€ì´ë“œ: ì¹­ì°¬ê³¼ ê²©ë ¤ë¥¼ ì•„ë¼ì§€ ë§ˆì„¸ìš”!)
ì„¤ëª…: ê°€ì¡± ì‹ì‚¬ í›„ì— ìë…€ì™€ í•¨ê»˜ ì„¤ê±°ì§€ì§€ë¥¼ í•˜ë©´ì„œ ì±…ì„ê°ì„ ê¸¸ëŸ¬ì£¼ì„¸ìš”. ë¶€ëª¨ë‹˜ê»˜ì„œëŠ” ì˜†ì—ì„œ ì¹­ì°¬ê³¼ ê²©ë ¤ë¥¼ í†µí•´ ìë…€ê°€ ì„±ì·¨ê°ì„ ëŠë¼ë„ë¡ ë„ì™€ì£¼ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤. ğŸ˜Š
ê¸°ëŒ€ íš¨ê³¼: ì„±ì·¨ê° ì¦ê°€, ê°€ì¡±ê³¼ì˜ ê¸ì •ì ì¸ ìƒí˜¸ì‘ìš© ì¦ì§„
{...}

ê²°ë¡  ë° ì¡°ì–¸
ìë…€ì˜ ê´€ì‹¬ì‚¬ì™€ ì„±í–¥ì„ ê³ ë ¤í•˜ì—¬, ìœ„ í™œë™ë“¤ì„ ê°€ì •ì—ì„œ ì‹¤ì²œí•´ ë³´ì„¸ìš”. ì–µì§€ë¡œ ê°•ìš”í•˜ê¸°ë³´ë‹¤ëŠ”, ì¹­ì°¬ê³¼ ê²©ë ¤ë¥¼ í†µí•´ ìë…€ê°€ ìŠ¤ìŠ¤ë¡œ ì°¸ì—¬í•˜ê³  ì¦ê±°ì›€ì„ ëŠë‚„ ìˆ˜ ìˆë„ë¡ ì§€ì§€í•´ ì£¼ì‹œëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë¶€ëª¨ë‹˜ì˜ ë”°ëœ»í•œ ê´€ì‹¬ê³¼ ì§€ì§€ëŠ” ìë…€ì˜ ê±´ê°•í•œ ì„±ì¥ì— í° í˜ì´ ë  ê²ƒì…ë‹ˆë‹¤. ğŸ‘
"""

NORMAL_SYSTEM_PROMPT = """
You are "ë…¸ë¯¸" who an expert in adolescent behavior change, specializing in guiding parents of 10-15-year-olds. Generate an informed response while following these guidelines
### Response Guidelines
- Adjust the wording naturally in your response (e.g., "my kid" â†’ "the child").
- Maintain a **warm, supportive, and informative tone** while showing respect for the parent's concerns.
- **Respond in Korean.**
{chat_history}
Parent: {input}
Nomi:
{context}
"""

class RAG_Chatbot:
    def __init__(
        self,
        embeddings_model_name="paraphrase-multilingual-mpnet-base-v2",
        folder_path="./data/crawled_data",
        vectordb_type="chroma",  # vectordb_type íŒŒë¼ë¯¸í„° ì¶”ê°€
        max_tokens=1024,
        openai_model_name="gpt-3.5-turbo-instruct"  # OpenAI ëª¨ë¸ ì´ë¦„ ì¶”ê°€
    ):
        """class ì´ˆê¸°í™”

        Args:
            embeddings_model_name (str, optional): ì„ë² ë”© ëª¨ë¸ ì´ë¦„. Defaults to "paraphrase-multilingual-mpnet-base-v2".
            folder_path (str, optional): RAGì˜ ë¬¸ì„œê°€ ë  ë°ì´í„°. Defaults to "./data/crawled_data".
            vectordb_type (str, optional): ì‚¬ìš©í•  VectorDB ìœ í˜• ("chroma" ë˜ëŠ” "faiss"). Defaults to "chroma".
        """
        self.embeddings = SentenceTransformerEmbeddings(
            model_name=embeddings_model_name
        )

        # ì €ì¥ ê²½ë¡œëŠ” DB ìœ í˜•ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •
        self.persist_directory = os.path.join(CURRENT_PATH, vectordb_type)
        self.folder_path = os.path.join(CURRENT_PATH, folder_path)
        self.vectordb_type = vectordb_type.lower()  # ì†Œë¬¸ìë¡œ í†µì¼
        self._make_vectordb()
        self.chat_model = ChatOpenAI(model_name="gpt-4o")
        self.llm = OpenAI(model_name=openai_model_name, max_tokens=max_tokens)  # OpenAI ëª¨ë¸ ì§€ì •
        self.memory = ConversationBufferMemory(memory_key="chat_history", k=5)  # ëŒ€í™” ê¸°ë¡ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.prompt = PromptTemplate(
            input_variables=["chat_history", "input", "context"], template=SYSTEM_PROMPT
        )
        self.normal_prompt = PromptTemplate(
            input_variables=["chat_history", "input", "context"], template=NORMAL_SYSTEM_PROMPT
        )
        

    @staticmethod
    def _update_prompt(prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

        Args:
            prompt (str): ì—…ë°ì´íŠ¸í•  í”„ë¡¬í”„íŠ¸.

        Returns:
            str: ì—…ë°ì´íŠ¸ëœ í”„ë¡¬í”„íŠ¸.
        """
        global SYSTEM_PROMPT
        SYSTEM_PROMPT = prompt
        return SYSTEM_PROMPT
    
    def _make_vectordb(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """VectorDBë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            chunk_size (int, optional): ì²­í¬ í¬ê¸°. Defaults to 500.
            chunk_overlap (int, optional): ì²­í¬ ì˜¤ë²„ë© í¬ê¸°. Defaults to 50.
            batch_size (int, optional): ë°°ì¹˜ í¬ê¸°. Defaults to 256.
        """

        print(self.vectordb_type, os.path.exists(self.persist_directory))

        if self.vectordb_type == "chroma":
            if os.path.exists(self.persist_directory):  # DBê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´
                print(f"ChromaDBê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {self.persist_directory}")
                self.vectordb = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings,
                )  # ì¡´ì¬í•˜ëŠ” DBë¥¼ ë¡œë“œí•˜ê³  ë°˜í™˜
                return

        elif self.vectordb_type == "faiss":
            if os.path.exists(self.persist_directory):  # DBê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´
                print(f"FAISSê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {self.persist_directory}")
                self.vectordb = FAISS.load_local(
                    self.persist_directory, self.embeddings
                )
                return

        nltk.download("averaged_perceptron_tagger_eng", download_dir="/nltk_data")

        # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
        loader = DirectoryLoader(self.folder_path, glob="./*.txt")
        documents = loader.load()

        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )
        texts = text_splitter.split_documents(documents)

        self.vectordb = self._add_to_vectordb(
            texts, self.embeddings, persist_directory=self.persist_directory
        )

    def _add_to_vectordb(
        self,
        texts: list,
    ):
        """ë¬¸ì„œë“¤ì„ VectorDBì— ì¶”ê°€í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤. ì´ë¯¸ DBê°€ ì¡´ì¬í•˜ë©´ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

        Args:
            texts (list): langchain Document ê°ì²´ì˜ ë¦¬ìŠ¤íŠ¸.
            embeddings (SentenceTransformerEmbeddings): ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸.
            persist_directory (str): VectorDB ë°ì´í„°ë¥¼ ì €ì¥í•  ê²½ë¡œ.

        Returns:
            Chroma | FAISS: VectorDB ì¸ìŠ¤í„´ìŠ¤.
        """

        if self.vectordb_type == "chroma":
            db = Chroma.from_documents(
                texts, self.embeddings, persist_directory=self.persist_directory
            )  # ì´ˆê¸° ChromaDB ìƒì„±
            db.persist()  # ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥
            return db

        elif self.vectordb_type == "faiss":
            db = FAISS.from_documents(texts, self.embeddings)
            db.save_local(self.persist_directory)
            return db

        else:
            raise ValueError(
                f"Invalid vectordb_type: {self.vectordb_type}. Choose 'chroma' or 'faiss'."
            )

    def get_chat(self, query: str) -> str:
        # retriever = self.vectordb.as_retriever(search_kwargs={"k": 5})
        docs_and_scores = self.vectordb.similarity_search_with_score(query, k=3)
        source_documents = [doc.page_content for doc, _ in docs_and_scores]
        similarities = [1 / (score + 1) for _, score in docs_and_scores]
        context = "\n".join(source_documents) if source_documents else ""

        print("=== Memory Contents ===")
        print(self.memory.load_memory_variables({}))
        print("=======================")

        print(similarities)

        memory_variables = self.memory.load_memory_variables({})
        chat_history = memory_variables.get("chat_history", "no caht")  # chat_historyê°€ ì—†ì„ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ì‚¬ìš©
        print('chat_history', chat_history)

        print(self.prompt)

        # ì„ê³„ê°’ì„ ë„˜ì§€ ëª»í•˜ê±°ë‚˜ sourceë¥¼ ì°¾ì§€ ëª»í–ˆì„ ë•Œ
        if max(similarities) < 0.1 or not source_documents:
            print('ì¼ë°˜ì ì¸ ëŒ€ë‹µ')
            final_prompt = self.prompt.format(input=query, chat_history=chat_history, context="no context")
        else:
            print('RAG ëŒ€ë‹µ')
            final_prompt = self.prompt.format(input=query, chat_history=chat_history, context=context)
        
        result = self.chat_model.predict(final_prompt)
        self.memory.save_context({"input": query}, {"output": result})  # ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        return result

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="RAG Chatbot with ChromaDB or FAISS")
    parser.add_argument(
        "--build_db",
        action="store_true",
        help="Build VectorDB during Docker image build",
    )
    parser.add_argument(
        "--vectordb_type",
        type=str,
        default="chroma",
        choices=["chroma", "faiss"],
        help="VectorDB type: chroma or faiss",
    )  # vectordb_type ì¸ì ì¶”ê°€
    args = parser.parse_args()

    chatbot = RAG_Chatbot(vectordb_type=args.vectordb_type)  # ì¸ì ì „ë‹¬

    if args.build_db:
        print(f"Building {args.vectordb_type.capitalize()}...")
        chatbot._make_vectordb()
        print(
            f"{args.vectordb_type.capitalize()} built and saved to {chatbot.persist_directory}"
        )
    else:
        query = """
        12ì‚´ ë”¸ì„ í‚¤ìš°ê³  ìˆëŠ”ë°, ìš°ë¦¬ ë”¸ì´ ê°„ì‹ì„ ë„ˆë¬´ ì¢‹ì•„í•´
        ë¬¸ì œëŠ” ìš´ë™ë„ ì•ˆí•˜ê³  ì“°ë ˆê¸°ë„ ì¹˜ìš°ì§€ ì•Šì•„
        ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œ?
        """
        answer = chatbot.get_chat(query)
        print(answer)